{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo for SPEL using LakeTemperature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unittests_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Define Unit Test parameters \u001b[39;00m\n\u001b[1;32m      2\u001b[0m casename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLakeTemp\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Name of the test case\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m case_dir \u001b[38;5;241m=\u001b[39m \u001b[43munittests_dir\u001b[49m \u001b[38;5;241m+\u001b[39m casename \u001b[38;5;66;03m# Directory to store the test case\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# List of subroutines to be analyzed\u001b[39;00m\n\u001b[1;32m      6\u001b[0m sub_name_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLakeTemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unittests_dir' is not defined"
     ]
    }
   ],
   "source": [
    "%tb\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# Built-in modules\n",
    "import re # Regular expressions\n",
    "import importlib # MUST BE USED TO RELOAD MODULES\n",
    "\n",
    "# Importing modules for SPEL functions \n",
    "import utilityFunctions as uf\n",
    "import edit_files as ef \n",
    "import fortran_modules as fm \n",
    "from analyze_subroutines import Subroutine\n",
    "# mod_config : system configuration and static variables. \n",
    "# Where E3SM is stored, where unit tests are stored, etc.\n",
    "from mod_config import default_mods, unittests_dir, scripts_dir, spel_mods_dir\n",
    "from mod_config import ELM_SRC, spel_output_dir, _bc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPEL needs a \"casename\" (what your unit-test is called) and a list of subroutines to create a unit-test for \"sub_name_list\".  For this demo, the sub_name_list only has \"LakeTemperature\" in it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Unit Test parameters \n",
    "casename = \"LakeTemp\"  # Name of the test case\n",
    "case_dir = unittests_dir + casename # Directory to store the test case\n",
    "\n",
    "# List of subroutines to be analyzed\n",
    "sub_name_list = [\"LakeTemperature\"]\n",
    "sub_name_list = [sub.lower() for sub in sub_name_list]\n",
    "\n",
    "# variables needed for Unit Test\n",
    "main_sub_dict = {}  # Dictionary to store all Subroutines in files needed for Unit Test\n",
    "subroutines = {k.lower():[] for k in sub_name_list} # Dictionary for User Specified Subroutines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1st step for SPEL is determine which modules are needed for LakeTemperature and edit out the I/O, MPI, and other unneccessary modules. This must be somewhat tailored to ELM.\n",
    "\n",
    "Currently if I module is not present in the \"components/elm/src/\" (ELM_SRC) nor in the \"share/utils\" directories (SHR_SRC), then the module and any dependency on it is automatically removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find shr_megan_mod in ELM or shared source -- adding to removal list\n",
      "Couldn't find shr_fan_mod in ELM or shared source -- adding to removal list\n",
      "Couldn't find petscsys in ELM or shared source -- adding to removal list\n",
      "Couldn't find petscvec in ELM or shared source -- adding to removal list\n",
      "Couldn't find petscmat in ELM or shared source -- adding to removal list\n",
      "Couldn't find petscts in ELM or shared source -- adding to removal list\n",
      "Couldn't find petscdm in ELM or shared source -- adding to removal list\n",
      "Couldn't find petscdmda in ELM or shared source -- adding to removal list\n",
      "Couldn't find unstructuredgridtype in ELM or shared source -- adding to removal list\n",
      "Couldn't find mathfuncmod in ELM or shared source -- adding to removal list\n",
      "Couldn't find tracer_varcon in ELM or shared source -- adding to removal list\n",
      "Couldn't find fatesinterfacetypesmod in ELM or shared source -- adding to removal list\n"
     ]
    }
   ],
   "source": [
    " # List to hold all the modules needed for the unit test\n",
    "needed_mods = [] \n",
    "for s in sub_name_list:\n",
    "    # Get general info of the subroutine\n",
    "    subroutines[s] = Subroutine(s,calltree=['elm_drv'])\n",
    "\n",
    "    # Process by removing certain modules and syntax\n",
    "    # so that a standalone unit test can be compiled.\n",
    "    fn = subroutines[s].filepath\n",
    "    mod_dict, file_list = ef.process_for_unit_test(fname=fn,case_dir=case_dir,\n",
    "                            mods=needed_mods,required_mods=default_mods, \n",
    "                            main_sub_dict=main_sub_dict,\n",
    "                            overwrite=False,verbose=False)\n",
    "    subroutines[s] = main_sub_dict[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary containing all identified user defined types\n",
    "# 'type-name' : 'DerivedType Object'\n",
    "type_dict = {}\n",
    "for modname, mod in mod_dict.items():\n",
    "    for utype, dtype in mod.defined_types.items():\n",
    "        type_dict[utype] = dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPEL can print out a module tree showing how the modules are linked for the unit-test subroutines\n",
    "- The full tree can be pretty difficult to parse, having a cutoff depth is recommended.\n",
    "- Every module uses 'shr_kind_mod' so that could be suppressed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(fm)\n",
    "modtree = fm.print_spel_module_dependencies(mod_dict=mod_dict,subs=subroutines)\n",
    "\n",
    "arrow = \"-->\"\n",
    "cutoff_depth = 10 # Only print modules up to this depth\n",
    "suppress_mod_list = ['shr_kind_mod']\n",
    "for m in modtree:\n",
    "    depth = m['depth']\n",
    "    modname = m['module']\n",
    "    if(modname in suppress_mod_list):\n",
    "        continue\n",
    "    if(depth == 1):\n",
    "        print(_bc.HEADER + arrow*depth + modname + _bc.ENDC)\n",
    "    elif(depth <= cutoff_depth):\n",
    "        print( arrow*depth + modname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mod = mod_dict['laketemperaturemod']\n",
    "test_mod.display_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in file_list:\n",
    "    base_fn = f.split('/')[-1]\n",
    "    print(base_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sub in main_sub_dict.values():\n",
    "#     print(sub.name)\n",
    "sub = main_sub_dict['laketemperature']\n",
    "sub.printSubroutineInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.LocalVariables['arrays']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mResolving interface for tridiagonal\n",
      " with args: ['bounds', '-nlevsno + 1', 'nlevlak + nlevgrnd', 'jtop', 'num_lakec', 'filter_lakec', 'a', 'b', 'c1', 'r', 'tx']\n",
      "/home/mrgex/SPEL_Openacc/scripts/../../repo/E3SM/components/elm/src/biogeophys/TridiagonalMod.F90 16   interface Tridiagonal\u001b[0m\n",
      "Couldn't match -nlevsno + 1 to any known variable -- assuming xx type\n",
      "Couldn't match nlevlak + nlevgrnd to any known variable -- assuming xx type\n",
      "tridiagonal_sr:: 1 Optional arguments found\n",
      "bounds matches bounds\n",
      "-nlevsno + 1 matches lbj\n",
      "nlevlak + nlevgrnd matches ubj\n",
      "jtop matches jtop\n",
      "num_lakec matches numf\n",
      "filter_lakec matches filter\n",
      "a matches a\n",
      "b matches b\n",
      "c1 matches c\n",
      "r matches r\n",
      "tx matches u\n",
      "resolve_interface::Subroutine is tridiagonal_sr\u001b[0m\n",
      "_preprocess_file::New child sub name is: tridiagonal_sr\n",
      "_preprocess_file::Finished analyzing for laketemperature\n"
     ]
    }
   ],
   "source": [
    "for s in sub_name_list:\n",
    "    # Parsing means getting info on the variables read and written\n",
    "    # to by the subroutine and any of its callees\n",
    "    subroutines[s].parse_subroutine(dtype_dict=type_dict,\n",
    "                                    main_sub_dict=main_sub_dict,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing child subroutine: soilthermprop_lake\n",
      "_preprocess_file::Finished analyzing for soilthermprop_lake\n",
      "Analyzing child subroutine: tridiagonal_sr\n",
      "_preprocess_file::Finished analyzing for tridiagonal_sr\n",
      "Analyzing child subroutine: phasechange_lake\n",
      "_preprocess_file::Finished analyzing for phasechange_lake\n",
      "\u001b[92mDerived Type Analysis for laketemperature\n",
      "Read-Only\n",
      "bounds%begc r\n",
      "bounds%endc r\n",
      "col_pp%snl r\n",
      "veg_pp%column r\n",
      "solarabs_vars%fsds_nir_d_patch r\n",
      "solarabs_vars%fsds_nir_i_patch r\n",
      "solarabs_vars%fsr_nir_d_patch r\n",
      "solarabs_vars%fsr_nir_i_patch r\n",
      "solarabs_vars%sabg_patch r\n",
      "col_pp%z_lake r\n",
      "lakestate_vars%ws_col r\n",
      "lakestate_vars%ks_col r\n",
      "col_pp%z r\n",
      "col_es%t_grnd r\n",
      "col_pp%lakedepth r\n",
      "lakestate_vars%etal_col r\n",
      "col_pp%dz_lake r\n",
      "col_pp%dz r\n",
      "lakestate_vars%lake_raw_col r\n",
      "soilstate_vars%tksatu_col r\n",
      "soilstate_vars%tkmg_col r\n",
      "soilstate_vars%watsat_col r\n",
      "soilstate_vars%tkdry_col r\n",
      "col_pp%zi r\n",
      "soilstate_vars%csol_col r\n",
      "Write-Only\n",
      "col_ws%frac_iceold w\n",
      "lakestate_vars%savedtke1_col w\n",
      "col_ef%eflx_snomelt w\n",
      "col_ef%imelt w\n",
      "Read-Write\n",
      "col_es%hc_soi rw\n",
      "lakestate_vars%lakeresist_col rw\n",
      "veg_ef%eflx_gnet rw\n",
      "lakestate_vars%betaprime_col rw\n",
      "lakestate_vars%lake_icefrac_col rw\n",
      "col_es%t_lake rw\n",
      "col_es%t_soisno rw\n",
      "ch4_vars%grnd_ch4_cond_col rw\n",
      "col_ef%errsoi rw\n",
      "veg_ef%eflx_sh_tot rw\n",
      "veg_ef%eflx_sh_grnd rw\n",
      "veg_ef%eflx_soil_grnd rw\n",
      "lakestate_vars%lake_icethick_col rw\n",
      "col_wf%qflx_snomelt rw\n",
      "col_wf%qflx_snow_melt rw\n",
      "col_wf%qflx_snofrz_lyr rw\n",
      "col_ws%h2osno rw\n",
      "col_ws%snow_depth rw\n",
      "col_ws%h2osoi_ice rw\n",
      "col_ws%h2osoi_liq rw\n",
      "col_wf%qflx_snofrz rw\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "read_types  = [] \n",
    "write_types = []\n",
    "\n",
    "for s in sub_name_list:\n",
    "    subroutines[s].child_subroutines_analysis(dtype_dict=type_dict,\n",
    "                                        main_sub_dict=main_sub_dict,verbose=True)\n",
    "    print(_bc.OKGREEN+f\"Derived Type Analysis for {subroutines[s].name}\")\n",
    "    print(f\"Read-Only\")\n",
    "    for key in subroutines[s].elmtype_r.keys():\n",
    "        print(key, subroutines[s].elmtype_r[key])\n",
    "    print(f\"Write-Only\")\n",
    "    for key in subroutines[s].elmtype_w.keys():\n",
    "        print(key, subroutines[s].elmtype_w[key])\n",
    "    print(f\"Read-Write\")\n",
    "    for key in subroutines[s].elmtype_rw.keys():\n",
    "        print(key, subroutines[s].elmtype_rw[key])\n",
    "    print(_bc.ENDC)\n",
    "    for key in subroutines[s].elmtype_r.keys():\n",
    "        c13c14 = bool('c13' in key or 'c14' in key)\n",
    "        if(c13c14):\n",
    "            del subroutines[s].elmtype_r[key]\n",
    "            continue\n",
    "        if(\"_inst\" in key):\n",
    "           print(f\"error: {key} has _inst\")\n",
    "           sys.exit(1)\n",
    "        read_types.append(key)\n",
    "        \n",
    "    for key in subroutines[s].elmtype_w.keys():\n",
    "        c13c14 = bool('c13' in key or 'c14' in key)\n",
    "        if(c13c14):\n",
    "            del subroutines[s].elmtype_w[key]\n",
    "            continue\n",
    "        if(\"_inst\" in key):\n",
    "            print(f\"error: {key} has _inst\")\n",
    "            sys.exit(1)\n",
    "        write_types.append(key)\n",
    "        \n",
    "    for key in subroutines[s].elmtype_rw.keys():\n",
    "        c13c14 = bool('c13' in key or 'c14' in key)\n",
    "        if(c13c14):\n",
    "            del subroutines[s].elmtype_w[key]\n",
    "            continue\n",
    "        write_types.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_types: ['bounds%begc', 'bounds%endc', 'col_pp%snl', 'veg_pp%column', 'solarabs_vars%fsds_nir_d_patch', 'solarabs_vars%fsds_nir_i_patch', 'solarabs_vars%fsr_nir_d_patch', 'solarabs_vars%fsr_nir_i_patch', 'solarabs_vars%sabg_patch', 'col_pp%z_lake', 'lakestate_vars%ws_col', 'lakestate_vars%ks_col', 'col_pp%z', 'col_es%t_grnd', 'col_pp%lakedepth', 'lakestate_vars%etal_col', 'col_pp%dz_lake', 'col_pp%dz', 'lakestate_vars%lake_raw_col', 'soilstate_vars%tksatu_col', 'soilstate_vars%tkmg_col', 'soilstate_vars%watsat_col', 'soilstate_vars%tkdry_col', 'col_pp%zi', 'soilstate_vars%csol_col']\n",
      "write_types: ['col_ws%frac_iceold', 'lakestate_vars%savedtke1_col', 'col_ef%eflx_snomelt', 'col_ef%imelt', 'col_es%hc_soi', 'lakestate_vars%lakeresist_col', 'veg_ef%eflx_gnet', 'lakestate_vars%betaprime_col', 'lakestate_vars%lake_icefrac_col', 'col_es%t_lake', 'col_es%t_soisno', 'ch4_vars%grnd_ch4_cond_col', 'col_ef%errsoi', 'veg_ef%eflx_sh_tot', 'veg_ef%eflx_sh_grnd', 'veg_ef%eflx_soil_grnd', 'lakestate_vars%lake_icethick_col', 'col_wf%qflx_snomelt', 'col_wf%qflx_snow_melt', 'col_wf%qflx_snofrz_lyr', 'col_ws%h2osno', 'col_ws%snow_depth', 'col_ws%h2osoi_ice', 'col_ws%h2osoi_liq', 'col_wf%qflx_snofrz']\n",
      "list of global vars: ['bounds', 'col_pp', 'veg_pp', 'solarabs_vars', 'lakestate_vars', 'col_es', 'soilstate_vars', 'col_ws', 'col_ef', 'veg_ef', 'ch4_vars', 'col_wf']\n"
     ]
    }
   ],
   "source": [
    "# Make sure physical properties types are read/written:\n",
    "list_pp = ['veg_pp','lun_pp','col_pp','grc_pp','top_pp']\n",
    "\n",
    "print(\"read_types:\",read_types)\n",
    "print(\"write_types:\",write_types)\n",
    "\n",
    "aggregated_elmtypes_list = []\n",
    "for x in read_types:\n",
    "    dtype_inst = x.split('%')[0]\n",
    "    if(dtype_inst not in aggregated_elmtypes_list):\n",
    "        aggregated_elmtypes_list.append(dtype_inst)    \n",
    "for x in write_types:\n",
    "    dtype_inst = x.split('%')[0]\n",
    "    if(dtype_inst not in aggregated_elmtypes_list):\n",
    "        aggregated_elmtypes_list.append(dtype_inst)\n",
    "\n",
    "# for l in list_pp:\n",
    "#     aggregated_elmtypes_list.append(l)\n",
    "print(\"list of global vars:\",aggregated_elmtypes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['col_pp', 'snl', 'integer', '1D'], ['veg_pp', 'column', 'integer', '1D'], ['solarabs_vars', 'fsds_nir_d_patch', 'real', '1D'], ['solarabs_vars', 'fsds_nir_i_patch', 'real', '1D'], ['solarabs_vars', 'fsr_nir_d_patch', 'real', '1D'], ['solarabs_vars', 'fsr_nir_i_patch', 'real', '1D'], ['solarabs_vars', 'sabg_patch', 'real', '1D'], ['col_pp', 'z_lake', 'real', '2D'], ['lakestate_vars', 'ws_col', 'real', '1D'], ['lakestate_vars', 'ks_col', 'real', '1D'], ['col_pp', 'z', 'real', '2D'], ['col_es', 't_grnd', 'real', '1D'], ['col_pp', 'lakedepth', 'real', '1D'], ['lakestate_vars', 'etal_col', 'real', '1D'], ['col_pp', 'dz_lake', 'real', '2D'], ['col_pp', 'dz', 'real', '2D'], ['lakestate_vars', 'lake_raw_col', 'real', '1D'], ['soilstate_vars', 'tksatu_col', 'real', '2D'], ['soilstate_vars', 'tkmg_col', 'real', '2D'], ['soilstate_vars', 'watsat_col', 'real', '2D'], ['soilstate_vars', 'tkdry_col', 'real', '2D'], ['col_pp', 'zi', 'real', '2D'], ['soilstate_vars', 'csol_col', 'real', '2D'], ['col_ws', 'frac_iceold', 'real', '2D'], ['lakestate_vars', 'savedtke1_col', 'real', '1D'], ['col_ef', 'eflx_snomelt', 'real', '1D'], ['col_ef', 'imelt', 'integer', '2D'], ['col_es', 'hc_soi', 'real', '1D'], ['lakestate_vars', 'lakeresist_col', 'real', '1D'], ['veg_ef', 'eflx_gnet', 'real', '1D'], ['lakestate_vars', 'betaprime_col', 'real', '1D'], ['lakestate_vars', 'lake_icefrac_col', 'real', '2D'], ['col_es', 't_lake', 'real', '2D'], ['col_es', 't_soisno', 'real', '2D'], ['ch4_vars', 'grnd_ch4_cond_col', 'real', '1D'], ['col_ef', 'errsoi', 'real', '1D'], ['veg_ef', 'eflx_sh_tot', 'real', '1D'], ['veg_ef', 'eflx_sh_grnd', 'real', '1D'], ['veg_ef', 'eflx_soil_grnd', 'real', '1D'], ['lakestate_vars', 'lake_icethick_col', 'real', '1D'], ['col_wf', 'qflx_snomelt', 'real', '1D'], ['col_wf', 'qflx_snow_melt', 'real', '1D'], ['col_wf', 'qflx_snofrz_lyr', 'real', '2D'], ['col_ws', 'h2osno', 'real', '1D'], ['col_ws', 'snow_depth', 'real', '1D'], ['col_ws', 'h2osoi_ice', 'real', '2D'], ['col_ws', 'h2osoi_liq', 'real', '2D'], ['col_wf', 'qflx_snofrz', 'real', '1D']]\n"
     ]
    }
   ],
   "source": [
    "from UnitTestforELM import set_active_variables\n",
    "instance_to_user_type = {}\n",
    "elm_inst_vars = {}\n",
    "for type_name, dtype in type_dict.items():\n",
    "    if('bounds' in type_name): \n",
    "        continue\n",
    "    if(not dtype.instances):\n",
    "        print(f\"Warning: no instances found for {type_name}\")\n",
    "        cmd = f'grep -rin -E \"^[[:space:]]*(type)[[:space:]]*\\({type_name}\" {ELM_SRC}/main/elm_instMod.F90'\n",
    "        output = sp.getoutput(cmd)\n",
    "        print(f\"output: {output}\")\n",
    "        if(output):\n",
    "            output = output.split('\\n')\n",
    "            if(len(output) > 1):\n",
    "                print(f\"Warning: multiple instances found for {type_name}\")\n",
    "                print(output)\n",
    "                sys.exit(1)\n",
    "            line = output[0]\n",
    "            line = line.replace('::','')\n",
    "            line = line.split(':')\n",
    "            \n",
    "            decl = line[1].strip()\n",
    "            decl = decl.split()\n",
    "            var = decl[1]\n",
    "            new_inst = Variable(type_name,var,subgrid='?',ln=0,dim=0,declaration='elm_instMod')\n",
    "            dtype.instances.append(new_inst)\n",
    "            elm_inst_vars[var] = dtype\n",
    "        else:\n",
    "            print(f\"Warning: no instances found for {type_name}\")\n",
    "    for instance in dtype.instances:\n",
    "        instance_to_user_type[instance.name] = type_name\n",
    "\n",
    "dtype_info_list = []\n",
    "    \n",
    "for s in sub_name_list:\n",
    "    set_active_variables(type_dict,instance_to_user_type,\n",
    "                            subroutines[s].elmtype_r,dtype_info_list)\n",
    "    set_active_variables(type_dict,instance_to_user_type,\n",
    "                            subroutines[s].elmtype_w,dtype_info_list)\n",
    "    set_active_variables(type_dict,instance_to_user_type,\n",
    "                            subroutines[s].elmtype_rw,dtype_info_list)\n",
    "    \n",
    "print(dtype_info_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
